{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spanberta-pretraining-bert-from-scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iMsVyvH6Bvv",
        "colab_type": "text"
      },
      "source": [
        "# SpanBERTa - Part I: How We Trained RoBERTa Language Model for Spanish from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnR5gCx9-WQB",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_QN4rLR9jC8",
        "colab_type": "text"
      },
      "source": [
        "- [Part II: Fine-tuning SpanBERTa for Named Entity Recognition](https://chriskhanhtran.github.io/posts/named-entity-recognition-with-transformers/)\n",
        "\n",
        "Self-training methods with transformer models have achieved state-of-the-art performance on most NLP tasks. However, because training them is computationally expensive, most currently available pretrained transformer models are only for English. Therefore, to improve performance in NLP tasks in our projects on Spanish, my team at [Skim AI](https://skimai.com/) decided to train a **RoBERTa** language model for Spanish from scratch and call it SpanBERTa.\n",
        "\n",
        "SpanBERTa has the same size as RoBERTa-base. We followed RoBERTa's training schema to train the model on 18 GB of [OSCAR](https://traces1.inria.fr/oscar/)'s Spanish corpus in 8 days using 4 Tesla P100 GPUs.\n",
        "\n",
        "In this blog post, we will walk through an end-to-end process to train a BERT-like language model from scratch using `transformers` and `tokenizers` libraries by Hugging Face. There is also a Google Colab notebook to run the codes in this article directly. You can also modify the notebook accordingly to train a BERT-like model for other languages or fine-tune it on your customized dataset.\n",
        "\n",
        "Before moving on, I want to express a huge thank to the Hugging Face team for making state-of-the-art NLP models accessible for everyone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmoXJKZi9kSQ",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pusg6iIo9mwL",
        "colab_type": "text"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKs_0Gy998vO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POgXAT5W-O0_",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYcNNEC1_C7g",
        "colab_type": "text"
      },
      "source": [
        "We pretrained SpanBERTa on [OSCAR](https://traces1.inria.fr/oscar/)'s Spanish corpus. The full size of the dataset is 150 GB and we used a portion of 18 GB to train.\n",
        "\n",
        "In this example, for simplicity, we will use a dataset of Spanish movie subtitles from [OpenSubtitles](https://www.opensubtitles.org/en/search). This dataset has a size of 5.4 GB and we will train on a subset of ~300 MB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_37ybc_P-X9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c8578b9f-0583-4ffb-ed7a-08fafee38bc8"
      },
      "source": [
        "import os\n",
        "\n",
        "# Download and unzip movie substitle dataset\n",
        "if not os.path.exists('data/dataset.txt'):\n",
        "  !wget \"https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/es.txt.gz\" -O dataset.txt.gz\n",
        "  !gzip -d dataset.txt.gz\n",
        "  !mkdir data\n",
        "  !mv dataset.txt data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-06 15:53:04--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/es.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1859673728 (1.7G) [application/gzip]\n",
            "Saving to: ‘dataset.txt.gz’\n",
            "\n",
            "dataset.txt.gz      100%[===================>]   1.73G  17.0MB/s    in 1m 46s  \n",
            "\n",
            "2020-04-06 15:54:51 (16.8 MB/s) - ‘dataset.txt.gz’ saved [1859673728/1859673728]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaUNSZG2AI_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4a1ca7bc-6c17-446f-f540-0c5f44c5290d"
      },
      "source": [
        "# Total number of lines and some random lines\n",
        "!wc -l data/dataset.txt\n",
        "!shuf -n 5 data/dataset.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179287150 data/dataset.txt\n",
            "Sabes, pensé que tenías más pelotas que para enfrentarme a través de mi hermano.\n",
            "Supe todos los encantamientos en todas las lenguas de los Elfos hombres y Orcos.\n",
            "Anteriormente en Blue Bloods:\n",
            "Y quiero que prometas que no habrá ningún trato con Daniel Stafford.\n",
            "Fue comiquísimo.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG56GRVuHGJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a subset of first 1,000,000 lines for training\n",
        "TRAIN_SIZE = 1000000 #@param {type:\"integer\"}\n",
        "!(head -n $TRAIN_SIZE data/dataset.txt) > data/train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V9gs3XHV7ey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a subset of next 10,000 lines for validation\n",
        "VAL_SIZE = 10000 #@param {type:\"integer\"}\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p data/dataset.txt) > data/dev.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tBBtS-kAs_a",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train a Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZXLfO2yEMAL",
        "colab_type": "text"
      },
      "source": [
        "The original BERT implementation uses a WordPiece tokenizer with a vocabulary of 32K subword units. This method, however, can introduce \"unknown\" tokens when processing rare words.\n",
        "\n",
        "In this implementation, we use a byte-level BPE tokenizer with a vocabulary of 50,265 subword units (same as RoBERTa-base). Using byte-level BPE makes it possible to learn a subword vocabulary of modest size that can encode any input without getting \"unknown\" tokens.\n",
        "\n",
        "Because `ByteLevelBPETokenizer` produces 2 files `[\"vocab.json\", \"merges.txt\"]` while `BertWordPieceTokenizer` produces only 1 file `vocab.txt`, it will cause an error if we use `BertWordPieceTokenizer` to load outputs of a BPE tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87Us5-wbCudj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9eeec996-ed2d-416f-e133-b3656e2eb055"
      },
      "source": [
        "%%time\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "path = \"data/train.txt\"\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=path,\n",
        "                vocab_size=50265,\n",
        "                min_frequency=2,\n",
        "                special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "\n",
        "# Save files to disk\n",
        "!mkdir -p \"models/roberta\"\n",
        "tokenizer.save(\"models/roberta\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 37s, sys: 1.02 s, total: 1min 38s\n",
            "Wall time: 1min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls1cM966HsGs",
        "colab_type": "text"
      },
      "source": [
        "Super fast! It takes only 2 minutes to train on 10 million lines.\n",
        "\n",
        "<img src=\"https://github.com/chriskhanhtran/spanish-bert/blob/master/img/train_tokenizers.gif?raw=true\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zqvXwmYLjYV",
        "colab_type": "text"
      },
      "source": [
        "# Traing Language Model from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K02FBkSCN68f",
        "colab_type": "text"
      },
      "source": [
        "## 1. Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBX7cEtOPbCe",
        "colab_type": "text"
      },
      "source": [
        "RoBERTa has exactly the same architecture as BERT. The only differences are:\n",
        "- RoBERTa uses a Byte-Level BPE tokenizer with a larger subword vocabulary (50k vs 32k).\n",
        "- RoBERTa implements dynamic word masking and drops next sentence prediction task.\n",
        "- RoBERTa's training hyperparameters.\n",
        "\n",
        "Other architecture configurations can be found in the documentation ([RoBERTa](https://huggingface.co/transformers/_modules/transformers/configuration_roberta.html#RobertaConfig), [BERT](https://huggingface.co/transformers/_modules/transformers/configuration_bert.html#BertConfig)).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faIaPSlMOH5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "config = {\n",
        "\t\"architectures\": [\n",
        "\t\t\"RobertaForMaskedLM\"\n",
        "\t],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"roberta\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 12,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": 50265\n",
        "}\n",
        "\n",
        "with open(\"models/roberta/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "tokenizer_config = {\"max_len\": 512}\n",
        "\n",
        "with open(\"models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPenZXGwOOlo",
        "colab_type": "text"
      },
      "source": [
        "## 2. Training Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb64aUvtSB6-",
        "colab_type": "text"
      },
      "source": [
        "| Hyperparam          | BERT-base | RoBERTa-base |\n",
        "|---------------------|:---------:|:------------:|\n",
        "|Sequence Length      | 128, 512  | 512          |\n",
        "|Batch Size           | 256       | 8K           |\n",
        "|Peak Learning Rate   | 1e-4      | 6e-4         |\n",
        "|Max Steps            | 1M        | 500K         |\n",
        "|Warmup Steps         | 10K       | 24K          |\n",
        "|Weight Decay         | 0.01      | 0.01         |\n",
        "|Adam $\\epsilon$      | 1e-6      | 1e-6         |\n",
        "|Adam $\\beta_1$       | 0.9       | 0.9          |\n",
        "|Adam $\\beta_2$       | 0.999     | 0.98         |\n",
        "|Gradient Clipping    | 0.0       | 0.0          |\n",
        "\n",
        "Note the batch size when training RoBERTa is 8000. Therefore, although RoBERTa-base was trained for 500K steps, its training computational cost is 16 times that of BERT-base. In the [RoBERTa paper](https://arxiv.org/pdf/1907.11692.pdf), it is shown that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Larger batch size can be obtained by tweaking `gradient_accumulation_steps`.\n",
        "\n",
        "Due to computational constraint, we followed BERT-base's training schema and trained our SpanBERTa model using 4 Tesla P100 GPUs for 200K steps in 8 days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9B0sPSzSyNf",
        "colab_type": "text"
      },
      "source": [
        "## 3. Start Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSG-NPI5XCt2",
        "colab_type": "text"
      },
      "source": [
        "We will train our model from scratch using [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), a script provided by Hugging Face, which will preprocess, tokenize the corpus and train the model on *Masked Language Modeling* task. The script is optimized to train on a single big corpus. Therefore, if your dataset is large and you want to split it to train sequentially, you will need to modify the script, or be ready to get a monster machine with high memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqNhz_sz8JKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "20f90b8c-607b-45de-acb8-a4c1a040fe52"
      },
      "source": [
        "# Update April 22, 2020: Hugging Face updated run_language_modeling.py script.\n",
        "# Please use this version which was before the update.\n",
        "!wget -c https://raw.githubusercontent.com/chriskhanhtran/spanish-bert/master/run_language_modeling.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-24 02:28:21--  https://raw.githubusercontent.com/chriskhanhtran/spanish-bert/master/run_language_modeling.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34328 (34K) [text/plain]\n",
            "Saving to: ‘run_language_modeling.py’\n",
            "\n",
            "\rrun_language_modeli   0%[                    ]       0  --.-KB/s               \rrun_language_modeli 100%[===================>]  33.52K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2020-04-24 02:28:21 (10.1 MB/s) - ‘run_language_modeling.py’ saved [34328/34328]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icyuG1pZ8KYJ",
        "colab_type": "text"
      },
      "source": [
        "**Important Arguments**\n",
        "- `--line_by_line` Whether distinct lines of text in the dataset are to be handled as distinct sequences. If each line in your dataset is long and has ~512 tokens or more, you should use this setting. If each line is short, the default text preprocessing will concatenate all lines, tokenize them and slit tokenized outputs into blocks of 512 tokens. You can also split your datasets into small chunks and preprocess them separately. 3GB of text will take ~50 minutes to process with the default `TextDataset` class.\n",
        "- `--should_continue` Whether to continue from latest checkpoint in output_dir.\n",
        "- `--model_name_or_path` The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\n",
        "- `--mlm` Train with masked-language modeling loss instead of language modeling.\n",
        "- `--config_name, --tokenizer_name` Optional pretrained config and tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new config.\n",
        "- `--per_gpu_train_batch_size` Batch size per GPU/CPU for training. Choose the largest number you can fit on your GPUs. You will see an error if your batch size is too large.\n",
        "- `--gradient_accumulation_steps` Number of updates steps to accumulate before performing a backward/update pass. You can use this trick to increase batch size. For example, if `per_gpu_train_batch_size = 16` and `gradient_accumulation_steps = 4`, your total train batch size will be 64.\n",
        "- `--overwrite_output_dir` Overwrite the content of the output directory.\n",
        "- `--no_cuda, --fp16, --fp16_opt_level` Arguments for training on GPU/CPU.\n",
        "- Other arguments are model paths and training hyperparameters.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTFxGYuZ7NO0",
        "colab_type": "text"
      },
      "source": [
        "It's highly recommended to include model type (eg. \"roberta\", \"bert\", \"gpt2\" etc.) in the model path because the script uses the [`AutoModels`](https://huggingface.co/transformers/model_doc/auto.html?highlight=automodels) class to guess the model's configuration using pattern matching on the provided path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9JX9k1oTJcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model paths\n",
        "MODEL_TYPE = \"roberta\" #@param [\"roberta\", \"bert\"]\n",
        "MODEL_DIR = \"models/roberta\" #@param {type: \"string\"}\n",
        "OUTPUT_DIR = \"models/roberta/output\" #@param {type: \"string\"}\n",
        "TRAIN_PATH = \"data/train.txt\" #@param {type: \"string\"}\n",
        "EVAL_PATH = \"data/dev.txt\" #@param {type: \"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8x5_vxhFlna",
        "colab_type": "text"
      },
      "source": [
        "For this example, we will train for only 25 steps on a Tesla P4 GPU provided by Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XCGILJpFk5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "62bae3ac-332f-42e5-fb34-bec109685f41"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Apr  6 15:59:35 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXCI0wTQS1vL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Command line\n",
        "cmd = \"\"\"python run_language_modeling.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --max_step 25 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 10 \\\n",
        "    --save_steps 10 \\\n",
        "    --logging_steps 2 \\\n",
        "    --seed 42\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoBqjQRuU6_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arguments for training from scratch. I turn off evaluate_during_training,\n",
        "#   line_by_line, should_continue, and model_name_or_path.\n",
        "train_params = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"model_type\": MODEL_TYPE,\n",
        "    \"config_name\": MODEL_DIR,\n",
        "    \"tokenizer_name\": MODEL_DIR,\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"eval_path\": EVAL_PATH,\n",
        "    \"do_eval\": \"--do_eval\",\n",
        "    \"evaluate_during_training\": \"\",\n",
        "    \"line_by_line\": \"\",\n",
        "    \"should_continue\": \"\",\n",
        "    \"model_name_or_path\": \"\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uBGZqcgT7xC",
        "colab_type": "text"
      },
      "source": [
        "If you are training on a virtual machine, you can install tensorboard to monitor the training process. Here is our [Tensorboard](https://tensorboard.dev/experiment/4wOFJBwPRBK9wjKE6F32qQ/#scalars) for training SpanBERTa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weh7hagGUR_m",
        "colab_type": "text"
      },
      "source": [
        "```sh\n",
        "pip install tensorboard==2.1.0\n",
        "tensorboard dev upload --logdir runs\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrhhF6H3dk78",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/chriskhanhtran/spanish-bert/blob/master/img/tensorboard-spanberta.JPG?raw=true\" width=\"400\">\n",
        "\n",
        "*After 200k steps, the loss reached 1.8 and the perplexity reached 5.2.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvynbMUkUhdy",
        "colab_type": "text"
      },
      "source": [
        "Now let's start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SyKveBQVC7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2aa6a686-b99e-4c40-b680-0f7605615f7b"
      },
      "source": [
        "!{cmd.format(**train_params)}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/06/2020 15:59:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/06/2020 15:59:41 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/config.json\n",
            "04/06/2020 15:59:41 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/06/2020 15:59:41 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/config.json\n",
            "04/06/2020 15:59:41 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/06/2020 15:59:41 - INFO - transformers.tokenization_utils -   Model name 'models/roberta' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'models/roberta' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/06/2020 15:59:41 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta/added_tokens.json. We won't load it.\n",
            "04/06/2020 15:59:41 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta/special_tokens_map.json. We won't load it.\n",
            "04/06/2020 15:59:41 - INFO - transformers.tokenization_utils -   loading file models/roberta/vocab.json\n",
            "04/06/2020 15:59:41 - INFO - transformers.tokenization_utils -   loading file models/roberta/merges.txt\n",
            "04/06/2020 15:59:41 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/06/2020 15:59:41 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/06/2020 15:59:41 - INFO - transformers.tokenization_utils -   loading file models/roberta/tokenizer_config.json\n",
            "04/06/2020 15:59:41 - INFO - __main__ -   Training new model from scratch\n",
            "04/06/2020 15:59:55 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='data/dev.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='models/roberta/output', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='models/roberta', train_data_file='data/train.txt', warmup_steps=10, weight_decay=0.01)\n",
            "04/06/2020 15:59:55 - INFO - __main__ -   Creating features from dataset file at data\n",
            "tcmalloc: large alloc 1247010816 bytes == 0xdc4be000 @  0x7f90488781e7 0x5acd6b 0x54345c 0x7f8fe2c541ae 0x7f8fe2c75a04 0x50ac25 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x58efc9 0x4c9546 0x5886f4 0x58892e 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9\n",
            "04/06/2020 16:04:43 - INFO - __main__ -   Saving features into cached file data/roberta_cached_lm_510_train.txt\n",
            "04/06/2020 16:04:46 - INFO - __main__ -   ***** Running training *****\n",
            "04/06/2020 16:04:46 - INFO - __main__ -     Num examples = 165994\n",
            "04/06/2020 16:04:46 - INFO - __main__ -     Num Epochs = 1\n",
            "04/06/2020 16:04:46 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "04/06/2020 16:04:46 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "04/06/2020 16:04:46 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "04/06/2020 16:04:46 - INFO - __main__ -     Total optimization steps = 25\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/41499 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/41499 [00:01<13:18:02,  1.15s/it]\u001b[A\n",
            "Iteration:   0% 2/41499 [00:01<11:26:47,  1.01it/s]\u001b[A\n",
            "Iteration:   0% 3/41499 [00:02<10:10:30,  1.13it/s]\u001b[A\n",
            "Iteration:   0% 4/41499 [00:03<9:38:10,  1.20it/s] \u001b[A\n",
            "Iteration:   0% 5/41499 [00:03<8:52:44,  1.30it/s]\u001b[A\n",
            "Iteration:   0% 6/41499 [00:04<8:22:47,  1.38it/s]\u001b[A\n",
            "Iteration:   0% 7/41499 [00:04<8:00:55,  1.44it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:   0% 8/41499 [00:05<8:03:40,  1.43it/s]\u001b[A\n",
            "Iteration:   0% 9/41499 [00:06<7:46:57,  1.48it/s]\u001b[A\n",
            "Iteration:   0% 10/41499 [00:06<7:35:35,  1.52it/s]\u001b[A\n",
            "Iteration:   0% 11/41499 [00:07<7:28:29,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 12/41499 [00:08<7:41:41,  1.50it/s]\u001b[A\n",
            "Iteration:   0% 13/41499 [00:08<7:34:28,  1.52it/s]\u001b[A\n",
            "Iteration:   0% 14/41499 [00:09<7:28:46,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 15/41499 [00:10<7:23:29,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 16/41499 [00:10<7:38:06,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 17/41499 [00:11<7:29:13,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 18/41499 [00:12<7:24:04,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 19/41499 [00:12<7:21:59,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 20/41499 [00:13<7:38:06,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 21/41499 [00:14<7:30:30,  1.53it/s]\u001b[A\n",
            "Iteration:   0% 22/41499 [00:14<7:27:33,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 23/41499 [00:15<7:21:41,  1.57it/s]\u001b[A\n",
            "Iteration:   0% 24/41499 [00:16<7:36:10,  1.52it/s]\u001b[A\n",
            "Iteration:   0% 25/41499 [00:16<7:28:21,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 26/41499 [00:17<7:22:58,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 27/41499 [00:17<7:20:41,  1.57it/s]\u001b[A\n",
            "Iteration:   0% 28/41499 [00:18<7:36:30,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 29/41499 [00:19<7:30:17,  1.53it/s]\u001b[A\n",
            "Iteration:   0% 30/41499 [00:19<7:24:42,  1.55it/s]\u001b[A\n",
            "Iteration:   0% 31/41499 [00:20<7:22:10,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 32/41499 [00:21<7:36:47,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 33/41499 [00:21<7:29:44,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 34/41499 [00:22<7:26:47,  1.55it/s]\u001b[A\n",
            "Iteration:   0% 35/41499 [00:23<7:23:42,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 36/41499 [00:23<7:37:20,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 37/41499 [00:24<7:31:11,  1.53it/s]\u001b[A\n",
            "Iteration:   0% 38/41499 [00:25<7:23:59,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 39/41499 [00:25<7:19:59,  1.57it/s]\u001b[A04/06/2020 16:05:12 - INFO - transformers.configuration_utils -   Configuration saved in models/roberta/output/checkpoint-10/config.json\n",
            "04/06/2020 16:05:14 - INFO - transformers.modeling_utils -   Model weights saved in models/roberta/output/checkpoint-10/pytorch_model.bin\n",
            "04/06/2020 16:05:14 - INFO - __main__ -   Saving model checkpoint to models/roberta/output/checkpoint-10\n",
            "04/06/2020 16:05:18 - INFO - __main__ -   Saving optimizer and scheduler states to models/roberta/output/checkpoint-10\n",
            "\n",
            "Iteration:   0% 40/41499 [00:31<26:12:48,  2.28s/it]\u001b[A\n",
            "Iteration:   0% 41/41499 [00:32<20:33:19,  1.78s/it]\u001b[A\n",
            "Iteration:   0% 42/41499 [00:33<16:32:48,  1.44s/it]\u001b[A\n",
            "Iteration:   0% 43/41499 [00:33<13:46:56,  1.20s/it]\u001b[A\n",
            "Iteration:   0% 44/41499 [00:34<12:06:50,  1.05s/it]\u001b[A\n",
            "Iteration:   0% 45/41499 [00:35<10:40:29,  1.08it/s]\u001b[A\n",
            "Iteration:   0% 46/41499 [00:35<9:39:29,  1.19it/s] \u001b[A\n",
            "Iteration:   0% 47/41499 [00:36<8:54:09,  1.29it/s]\u001b[A\n",
            "Iteration:   0% 48/41499 [00:37<8:42:25,  1.32it/s]\u001b[A\n",
            "Iteration:   0% 49/41499 [00:37<8:20:20,  1.38it/s]\u001b[A\n",
            "Iteration:   0% 50/41499 [00:38<8:00:26,  1.44it/s]\u001b[A\n",
            "Iteration:   0% 51/41499 [00:38<7:48:01,  1.48it/s]\u001b[A\n",
            "Iteration:   0% 52/41499 [00:39<7:55:05,  1.45it/s]\u001b[A\n",
            "Iteration:   0% 53/41499 [00:40<7:43:59,  1.49it/s]\u001b[A\n",
            "Iteration:   0% 54/41499 [00:40<7:37:02,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 55/41499 [00:41<7:32:17,  1.53it/s]\u001b[A\n",
            "Iteration:   0% 56/41499 [00:42<7:43:40,  1.49it/s]\u001b[A\n",
            "Iteration:   0% 57/41499 [00:42<7:36:18,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 58/41499 [00:43<7:28:49,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 59/41499 [00:44<7:24:53,  1.55it/s]\u001b[A\n",
            "Iteration:   0% 60/41499 [00:44<7:38:38,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 61/41499 [00:45<7:32:45,  1.53it/s]\u001b[A\n",
            "Iteration:   0% 62/41499 [00:46<7:27:01,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 63/41499 [00:46<7:27:16,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 64/41499 [00:47<7:44:31,  1.49it/s]\u001b[A\n",
            "Iteration:   0% 65/41499 [00:48<7:36:01,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 66/41499 [00:48<7:31:34,  1.53it/s]\u001b[A\n",
            "Iteration:   0% 67/41499 [00:49<7:28:30,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 68/41499 [00:50<7:40:54,  1.50it/s]\u001b[A\n",
            "Iteration:   0% 69/41499 [00:50<7:35:17,  1.52it/s]\u001b[A\n",
            "Iteration:   0% 70/41499 [00:51<7:27:24,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 71/41499 [00:52<7:23:51,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 72/41499 [00:52<7:40:37,  1.50it/s]\u001b[A\n",
            "Iteration:   0% 73/41499 [00:53<7:33:48,  1.52it/s]\u001b[A\n",
            "Iteration:   0% 74/41499 [00:54<7:29:04,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 75/41499 [00:54<7:28:32,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 76/41499 [00:55<7:45:53,  1.48it/s]\u001b[A\n",
            "Iteration:   0% 77/41499 [00:56<7:36:10,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 78/41499 [00:56<7:33:20,  1.52it/s]\u001b[A\n",
            "Iteration:   0% 79/41499 [00:57<7:33:13,  1.52it/s]\u001b[A04/06/2020 16:05:44 - INFO - transformers.configuration_utils -   Configuration saved in models/roberta/output/checkpoint-20/config.json\n",
            "04/06/2020 16:05:46 - INFO - transformers.modeling_utils -   Model weights saved in models/roberta/output/checkpoint-20/pytorch_model.bin\n",
            "04/06/2020 16:05:46 - INFO - __main__ -   Saving model checkpoint to models/roberta/output/checkpoint-20\n",
            "04/06/2020 16:05:55 - INFO - __main__ -   Saving optimizer and scheduler states to models/roberta/output/checkpoint-20\n",
            "\n",
            "Iteration:   0% 80/41499 [01:09<46:57:02,  4.08s/it]\u001b[A\n",
            "Iteration:   0% 81/41499 [01:10<35:03:56,  3.05s/it]\u001b[A\n",
            "Iteration:   0% 82/41499 [01:10<26:42:29,  2.32s/it]\u001b[A\n",
            "Iteration:   0% 83/41499 [01:11<20:51:17,  1.81s/it]\u001b[A\n",
            "Iteration:   0% 84/41499 [01:12<17:05:06,  1.49s/it]\u001b[A\n",
            "Iteration:   0% 85/41499 [01:12<14:08:27,  1.23s/it]\u001b[A\n",
            "Iteration:   0% 86/41499 [01:13<12:04:38,  1.05s/it]\u001b[A\n",
            "Iteration:   0% 87/41499 [01:13<10:41:45,  1.08it/s]\u001b[A\n",
            "Iteration:   0% 88/41499 [01:14<9:59:24,  1.15it/s] \u001b[A\n",
            "Iteration:   0% 89/41499 [01:15<9:10:43,  1.25it/s]\u001b[A\n",
            "Iteration:   0% 90/41499 [01:15<8:41:22,  1.32it/s]\u001b[A\n",
            "Iteration:   0% 91/41499 [01:16<8:16:39,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 92/41499 [01:17<8:15:58,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 93/41499 [01:17<8:01:26,  1.43it/s]\u001b[A\n",
            "Iteration:   0% 94/41499 [01:18<7:51:21,  1.46it/s]\u001b[A\n",
            "Iteration:   0% 95/41499 [01:19<7:39:10,  1.50it/s]\u001b[A\n",
            "Iteration:   0% 96/41499 [01:19<7:47:55,  1.47it/s]\u001b[A\n",
            "Iteration:   0% 97/41499 [01:20<7:38:16,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 98/41499 [01:21<7:33:13,  1.52it/s]\u001b[A\n",
            "Iteration:   0% 99/41499 [01:21<7:30:50,  1.53it/s]\u001b[A\n",
            "Iteration:   0% 100/41499 [01:22<7:44:51,  1.48it/s]\u001b[A\n",
            "Iteration:   0% 101/41499 [01:23<7:37:02,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 102/41499 [01:23<7:30:44,  1.53it/s]\u001b[A\n",
            "Iteration:   0% 103/41499 [01:25<9:30:53,  1.21it/s]\n",
            "Epoch:   0% 0/1 [01:25<?, ?it/s]\n",
            "04/06/2020 16:06:11 - INFO - __main__ -    global_step = 26, average loss = 9.355212138249325\n",
            "04/06/2020 16:06:11 - INFO - __main__ -   Saving model checkpoint to models/roberta/output\n",
            "04/06/2020 16:06:11 - INFO - transformers.configuration_utils -   Configuration saved in models/roberta/output/config.json\n",
            "04/06/2020 16:06:13 - INFO - transformers.modeling_utils -   Model weights saved in models/roberta/output/pytorch_model.bin\n",
            "04/06/2020 16:06:13 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/output/config.json\n",
            "04/06/2020 16:06:13 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/06/2020 16:06:13 - INFO - transformers.modeling_utils -   loading weights file models/roberta/output/pytorch_model.bin\n",
            "04/06/2020 16:06:17 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/output/config.json\n",
            "04/06/2020 16:06:17 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/06/2020 16:06:17 - INFO - transformers.tokenization_utils -   Model name 'models/roberta/output' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'models/roberta/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/06/2020 16:06:17 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta/output/added_tokens.json. We won't load it.\n",
            "04/06/2020 16:06:17 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/vocab.json\n",
            "04/06/2020 16:06:17 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/merges.txt\n",
            "04/06/2020 16:06:17 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/06/2020 16:06:17 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/special_tokens_map.json\n",
            "04/06/2020 16:06:17 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/tokenizer_config.json\n",
            "04/06/2020 16:06:18 - INFO - __main__ -   Evaluate the following checkpoints: ['models/roberta/output']\n",
            "04/06/2020 16:06:18 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/output/config.json\n",
            "04/06/2020 16:06:18 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/06/2020 16:06:18 - INFO - transformers.modeling_utils -   loading weights file models/roberta/output/pytorch_model.bin\n",
            "04/06/2020 16:06:23 - INFO - __main__ -   Creating features from dataset file at data\n",
            "04/06/2020 16:06:23 - INFO - __main__ -   Saving features into cached file data/roberta_cached_lm_510_dev.txt\n",
            "04/06/2020 16:06:23 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "04/06/2020 16:06:23 - INFO - __main__ -     Num examples = 156\n",
            "04/06/2020 16:06:23 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100% 39/39 [00:08<00:00,  4.41it/s]\n",
            "04/06/2020 16:06:32 - INFO - __main__ -   ***** Eval results  *****\n",
            "04/06/2020 16:06:32 - INFO - __main__ -     perplexity = tensor(6077.6812)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDf8zX0y5ACK",
        "colab_type": "text"
      },
      "source": [
        "## 4. Predict Masked Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gevTp0eUfAiW",
        "colab_type": "text"
      },
      "source": [
        "After training your language model, you can upload and share your model with the community. We have uploaded our SpanBERTa model to Hugging Face's server. Before evaluating the model on downstream tasks, let's see how it has learned to fill masked words given a context. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62wMhcySbetg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "%%time\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"chriskhanhtran/spanberta\",\n",
        "    tokenizer=\"chriskhanhtran/spanberta\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSA0FL4glNhQ",
        "colab_type": "text"
      },
      "source": [
        "I pick a sentence from Wikipedia's article about COVID-19.\n",
        "\n",
        "The original sentence is \"*Lavarse frecuentemente las manos con agua y jabón,*\" meaning \"*Frequently wash your hands with soap and water.*\"\n",
        "\n",
        "The masked word is **\"jabón\" (soap)** and the top 5 predictions are **soap, salt, steam, lemon** and **vinegar**. It is interesting that the model somehow learns that we should wash our hands with things that can kill bacteria or contain acid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c0w1Xt2bnqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "d8088532-6c71-423d-f40c-6d784ab2415a"
      },
      "source": [
        "fill_mask(\"Lavarse frecuentemente las manos con agua y <mask>.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.6469631195068359,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y jabón.</s>',\n",
              "  'token': 18493},\n",
              " {'score': 0.06074320897459984,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y sal.</s>',\n",
              "  'token': 619},\n",
              " {'score': 0.029787985607981682,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y vapor.</s>',\n",
              "  'token': 11079},\n",
              " {'score': 0.026410052552819252,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y limón.</s>',\n",
              "  'token': 12788},\n",
              " {'score': 0.017029203474521637,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y vinagre.</s>',\n",
              "  'token': 18424}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx2rILSs9lck",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJLtZ1JP9pQL",
        "colab_type": "text"
      },
      "source": [
        "We have walked through how to train a BERT language model for Spanish from scratch and seen that the model has learned properties of the language by trying to predict masked words given a context. You can also follow this article to fine-tune a pretrained BERT-like model on your customized dataset.\n",
        "\n",
        "Next, we will implement the pretrained models on downstream tasks including Sequence Classification, NER, POS tagging, and NLI, as well as compare the model's performance with some non-BERT models.\n",
        "\n",
        "Stay tuned for our next posts!"
      ]
    }
  ]
}