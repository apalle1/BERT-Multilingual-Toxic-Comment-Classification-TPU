{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":1,"outputs":[{"output_type":"stream","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  4264  100  4264    0     0  99162      0 --:--:-- --:--:-- --:--:-- 99162\nUpdating TPU and VM. This may take around 2 minutes.\nUpdating TPU runtime to pytorch-dev20200515 ...\nFound existing installation: torch 1.5.0\nUninstalling torch-1.5.0:\nDone updating TPU runtime: <Response [200]>\n  Successfully uninstalled torch-1.5.0\nFound existing installation: torchvision 0.6.0a0+82fd1c8\nUninstalling torchvision-0.6.0a0+82fd1c8:\n  Successfully uninstalled torchvision-0.6.0a0+82fd1c8\nCopying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n- [1 files][ 91.0 MiB/ 91.0 MiB]                                                \nOperation completed over 1 objects/91.0 MiB.                                     \nCopying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n| [1 files][119.5 MiB/119.5 MiB]                                                \nOperation completed over 1 objects/119.5 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n/ [1 files][  2.3 MiB/  2.3 MiB]                                                \nOperation completed over 1 objects/2.3 MiB.                                      \nProcessing ./torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==nightly+20200515) (1.18.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==nightly+20200515) (0.18.2)\n\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.6.0a0+bf2bbd9 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.3 which is incompatible.\u001b[0m\nInstalling collected packages: torch\nSuccessfully installed torch-1.6.0a0+bf2bbd9\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nProcessing ./torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: torch-xla\nSuccessfully installed torch-xla-1.6+2b2085a\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nProcessing ./torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly+20200515) (1.18.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly+20200515) (5.4.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==nightly+20200515) (0.18.2)\nInstalling collected packages: torchvision\nSuccessfully installed torchvision-0.7.0a0+a6073f0\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  libgfortran4 libopenblas-base\nThe following NEW packages will be installed:\n  libgfortran4 libomp5 libopenblas-base libopenblas-dev\n0 upgraded, 4 newly installed, 0 to remove and 17 not upgraded.\nNeed to get 8550 kB of archives.\nAfter this operation, 97.6 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgfortran4 amd64 7.5.0-3ubuntu1~18.04 [492 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-base amd64 0.2.20+ds-4 [3964 kB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-dev amd64 0.2.20+ds-4 [3860 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\nFetched 8550 kB in 0s (36.0 MB/s) \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libgfortran4:amd64.\n(Reading database ... 105807 files and directories currently installed.)\nPreparing to unpack .../libgfortran4_7.5.0-3ubuntu1~18.04_amd64.deb ...\nUnpacking libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSelecting previously unselected package libopenblas-base:amd64.\nPreparing to unpack .../libopenblas-base_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-base:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libopenblas-dev:amd64.\nPreparing to unpack .../libopenblas-dev_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-dev:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libomp5:amd64.\nPreparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\nUnpacking libomp5:amd64 (5.0.1-1) ...\nSetting up libomp5:amd64 (5.0.1-1) ...\nSetting up libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSetting up libopenblas-base:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3 to provide /usr/lib/x86_64-linux-gnu/libblas.so.3 (libblas.so.3-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so.3 to provide /usr/lib/x86_64-linux-gnu/liblapack.so.3 (liblapack.so.3-x86_64-linux-gnu) in auto mode\nSetting up libopenblas-dev:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode\nProcessing triggers for libc-bin (2.27-3ubuntu1) ...\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to use TPU's using PyTorch, we have to use PyTorch-XLA library\nimport warnings\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\n\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.utils.utils as xu\nimport torch_xla.test.test_utils as test_utils\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 224\nTRAIN_BATCH_SIZE = 64\nVALID_BATCH_SIZE = 8\nEPOCHS = 2\n\nPRETRAINED_BERT = \"bert-base-multilingual-uncased\"\nMODEL_PATH = \"mBERT.bin\" \nTRAINING_FILE_1 = \"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\" \nTRAINING_FILE_2 = \"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\"  \nVALIDATION_FILE = \"../input/jigsaw-multilingual-toxic-comment-classification/validation.csv\" \n# Importing pre-trained BERT tokenizer\nTOKENIZER = transformers.BertTokenizer.from_pretrained(\n    PRETRAINED_BERT,\n    do_lower_case=True\n)","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0bd13b01f44cbdbc9a0d5899024b0f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTDatasetTraining:\n  def __init__(self, comment_text, targets, tokenizer, max_length):\n    self.comment_text = comment_text\n    self.tokenizer = tokenizer\n    self.max_length = max_length\n    self.targets = targets\n\n  def __len__(self):\n    return len(self.comment_text)\n\n  def __getitem__(self, item):\n    comment_text = str(self.comment_text[item])\n    comment_text = \" \".join(comment_text.split())\n\n    inputs = self.tokenizer.encode_plus(\n        comment_text,\n        None,\n        add_special_tokens=True,\n        max_length=self.max_length,\n        pad_to_max_length = True\n    )\n\n    ids = inputs[\"input_ids\"]\n    token_type_ids = inputs[\"token_type_ids\"]\n    mask = inputs[\"attention_mask\"]\n    \n    return {\n        'ids': torch.tensor(ids, dtype=torch.long),\n        'mask': torch.tensor(mask, dtype=torch.long),\n        'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n        'targets': torch.tensor(self.targets[item], dtype=torch.float)\n    }","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n  def __init__(self, pretrained_bert):\n    super(BERTBaseUncased, self).__init__()\n    self.pretrained_bert = pretrained_bert  \n    # Importing pre-trained BERT model\n    self.bert = transformers.BertModel.from_pretrained(self.pretrained_bert) \n    self.bert_drop = nn.Dropout(0.3)\n    self.out = nn.Linear(768 * 2, 1)\n\n  def forward(self, ids, mask, token_type_ids):\n    o1, o2 = self.bert(ids,\n                       attention_mask=mask,\n                       token_type_ids=token_type_ids)\n    \n    apool = torch.mean(o1, 1)\n    mpool, _ = torch.max(o1, 1)\n    cat = torch.cat((apool, mpool), 1)\n\n    bo = self.bert_drop(cat)\n    output = self.out(bo)\n\n    return output","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n\ndef train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n  model.train()\n  for bi, d in enumerate(data_loader):\n    ids = d[\"ids\"]\n    mask = d[\"mask\"]\n    token_type_ids = d[\"token_type_ids\"]\n    targets = d[\"targets\"]\n\n    ids = ids.to(device, dtype=torch.long)\n    mask = mask.to(device, dtype=torch.long)\n    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n    targets = targets.to(device, dtype=torch.float)\n\n    optimizer.zero_grad()\n    outputs = model(\n        ids=ids,\n        mask=mask,\n        token_type_ids=token_type_ids\n    )\n\n    loss = loss_fn(outputs, targets)\n    if bi % 10 == 0:\n        xm.master_print(f'bi={bi}, loss={loss}')\n\n    loss.backward()\n\n    ####################################### CHANGE HAPPENS HERE #######################################################\n    xm.optimizer_step(optimizer)\n    ###################################################################################################################\n\n    if scheduler is not None:\n        scheduler.step()\n\n\ndef eval_loop_fn(data_loader, model, device):\n  model.eval()\n  fin_targets = []\n  fin_outputs = []\n  for bi, d in enumerate(data_loader):\n    ids = d[\"ids\"]\n    mask = d[\"mask\"]\n    token_type_ids = d[\"token_type_ids\"]\n    targets = d[\"targets\"]\n\n    ids = ids.to(device, dtype=torch.long)\n    mask = mask.to(device, dtype=torch.long)\n    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n    targets = targets.to(device, dtype=torch.float)\n\n    outputs = model(\n        ids=ids,\n        mask=mask,\n        token_type_ids=token_type_ids\n    )\n\n    targets_np = targets.cpu().detach().numpy().tolist()\n    outputs_np = outputs.cpu().detach().numpy().tolist()\n    fin_targets.extend(targets_np)\n    fin_outputs.extend(outputs_np)    \n\n  return fin_outputs, fin_targets","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BERTBaseUncased(pretrained_bert=PRETRAINED_BERT)\n\ndf1 = pd.read_csv(TRAINING_FILE_1, usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\ndf2 = pd.read_csv(TRAINING_FILE_2, usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\n\ndf_train = pd.concat([df1, df2], axis=0).reset_index(drop=True)\ndf_train = df_train.sample(frac=1).reset_index(drop=True).head(200000) \n\ndf_valid = pd.read_csv(VALIDATION_FILE, usecols=[\"comment_text\", \"toxic\"])\n\ndf_train = pd.concat([df_train, df_valid], axis=0).reset_index(drop=True)\ndf_train = df_train.sample(frac=1).reset_index(drop=True)","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0b4007198b647738f1d9829d1937d7e"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=672271273.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c7de4cc96ad4ec98d2edb7b80aa7073"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _run():\n    \n  train_dataset = BERTDatasetTraining(\n      comment_text=df_train.comment_text.values,\n      targets=df_train.toxic.values,\n      tokenizer=TOKENIZER,\n      max_length=MAX_LEN\n  )\n\n  # We have to use DistributedSampler to use TPU's. It will distribute the data on different TPU cores.\n  train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True)\n\n  train_data_loader = torch.utils.data.DataLoader(\n      train_dataset,\n      batch_size=TRAIN_BATCH_SIZE,\n      sampler=train_sampler,\n      drop_last=True,\n      num_workers=4\n  )\n\n  valid_dataset = BERTDatasetTraining(\n      comment_text=df_valid.comment_text.values,\n      targets=df_valid.toxic.values,\n      tokenizer=TOKENIZER,\n      max_length=MAX_LEN\n  )\n\n  valid_sampler = torch.utils.data.distributed.DistributedSampler(\n        valid_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False)\n\n  valid_data_loader = torch.utils.data.DataLoader(\n      valid_dataset,\n      batch_size=VALID_BATCH_SIZE,\n      sampler=valid_sampler,\n      drop_last=False,\n      num_workers=4\n  )\n\n  ##################################### Change occurs Here ####################################################################\n  device = xm.xla_device()\n  model.to(device)\n  #############################################################################################################################\n\n  param_optimizer = list(model.named_parameters())\n  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n  optimizer_grouped_parameters = [\n      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n  # Number of training steps will get divided my number of cores\n  lr = 0.4 * 1e-5 * xm.xrt_world_size()\n  num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n  xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n  optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n  scheduler = get_linear_schedule_with_warmup(\n      optimizer,\n      num_warmup_steps=0,\n      num_training_steps=num_train_steps\n  )\n \n  best_roc_auc = 0\n  ########################################## Change occur In this Loop #################################################################\n  for epoch in range(EPOCHS):\n    # train_data_loader has to be wrapped inside ParallelLoader \n    para_loader = pl.ParallelLoader(train_data_loader, [device])\n    train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n\n    para_loader = pl.ParallelLoader(valid_data_loader, [device])\n    o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n  ########################################################################################################################################  \n\n    roc_auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n    xm.master_print(f'AUC = {roc_auc}')\n    if roc_auc > best_roc_auc:\n      # Instead of using torch.save, we will be saving using xm.save\n      xm.save(model.state_dict(), MODEL_PATH)\n      best_roc_auc = roc_auc","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start training processes\ndef _multiprocessing_function(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run()\n\nFLAGS={}\nxmp.spawn(_multiprocessing_function, args=(FLAGS,), nprocs=8, start_method='fork')","execution_count":10,"outputs":[{"output_type":"stream","text":"num_train_steps = 812, world_size=8\nbi=0, loss=0.6500879526138306\nbi=10, loss=0.38799771666526794\nbi=20, loss=0.3137851357460022\nbi=30, loss=0.3322542607784271\nbi=40, loss=0.2917523980140686\nbi=50, loss=0.26058685779571533\nbi=60, loss=0.3470531404018402\nbi=70, loss=0.23828867077827454\nbi=80, loss=0.20857523381710052\nbi=90, loss=0.2835109531879425\nbi=100, loss=0.22540879249572754\nbi=110, loss=0.2524469792842865\nbi=120, loss=0.26607745885849\nbi=130, loss=0.21546605229377747\nbi=140, loss=0.20279334485530853\nbi=150, loss=0.21931655704975128\nbi=160, loss=0.25631076097488403\nbi=170, loss=0.1354462057352066\nbi=180, loss=0.3117203414440155\nbi=190, loss=0.16671505570411682\nbi=200, loss=0.2922610640525818\nbi=210, loss=0.20948097109794617\nbi=220, loss=0.30595624446868896\nbi=230, loss=0.30060774087905884\nbi=240, loss=0.2394719123840332\nbi=250, loss=0.21053344011306763\nbi=260, loss=0.19130487740039825\nbi=270, loss=0.2283458113670349\nbi=280, loss=0.2802594006061554\nbi=290, loss=0.15899936854839325\nbi=300, loss=0.2980979382991791\nbi=310, loss=0.24340108036994934\nbi=320, loss=0.21223483979701996\nbi=330, loss=0.10220245271921158\nbi=340, loss=0.21080631017684937\nbi=350, loss=0.2241077572107315\nbi=360, loss=0.19981491565704346\nbi=370, loss=0.22508113086223602\nbi=380, loss=0.21481908857822418\nbi=390, loss=0.22915980219841003\nbi=400, loss=0.2946484684944153\nAUC = 0.9688311266874351\nbi=0, loss=0.23655977845191956\nbi=10, loss=0.2334776371717453\nbi=20, loss=0.22573146224021912\nbi=30, loss=0.17704330384731293\nbi=40, loss=0.2037816196680069\nbi=50, loss=0.1911540925502777\nbi=60, loss=0.24182088673114777\nbi=70, loss=0.19616547226905823\nbi=80, loss=0.20191088318824768\nbi=90, loss=0.24239268898963928\nbi=100, loss=0.20873424410820007\nbi=110, loss=0.23589234054088593\nbi=120, loss=0.2298322170972824\nbi=130, loss=0.20622576773166656\nbi=140, loss=0.18924742937088013\nbi=150, loss=0.184349924325943\nbi=160, loss=0.27347275614738464\nbi=170, loss=0.14340078830718994\nbi=180, loss=0.2637017071247101\nbi=190, loss=0.1623903214931488\nbi=200, loss=0.27116674184799194\nbi=210, loss=0.16952158510684967\nbi=220, loss=0.2657669186592102\nbi=230, loss=0.2746252715587616\nbi=240, loss=0.2293906956911087\nbi=250, loss=0.19252996146678925\nbi=260, loss=0.1761987805366516\nbi=270, loss=0.226186141371727\nbi=280, loss=0.27401262521743774\nbi=290, loss=0.15517643094062805\nbi=300, loss=0.27942946553230286\nbi=310, loss=0.23122809827327728\nbi=320, loss=0.1858929991722107\nbi=330, loss=0.11071620881557465\nbi=340, loss=0.20701703429222107\nbi=350, loss=0.2258768230676651\nbi=360, loss=0.18457217514514923\nbi=370, loss=0.22340446710586548\nbi=380, loss=0.19680017232894897\nbi=390, loss=0.22992613911628723\nbi=400, loss=0.26211774349212646\nAUC = 0.9854620976116304\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}